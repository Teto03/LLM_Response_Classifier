## ğŸ§  Jailbreak LLM Response Classifier

### Detection and analysis of responses generated by language models under adversarial prompts

> Bachelorâ€™s Thesis Project - University of Milano-Bicocca
> **Francesco Bianchi** Â· A.Y. 2024/2025
> Supervisors: Prof. Claudio Ferretti, Prof.ssa Martina Saletta

---

## ğŸ“š Project Background and Motivation

Large Language Models (LLMs) have demonstrated remarkable capabilities in generating coherent and contextually relevant text. However, they are vulnerable to **jailbreak prompts**â€”carefully crafted inputs designed to bypass alignment safeguards and elicit unintended or harmful responses. Ensuring the reliability and safety of LLMs is critical, especially as they are deployed in sensitive applications.

This project addresses the challenge of **automatically detecting** when an LLM response results from a jailbreak prompt. By combining **supervised learning** (fine-tuned BERT classification) with **unsupervised analysis** (clustering), we aim to provide both accurate detection and deeper insights into the ways models fail alignment.

---

## ğŸš€ Key Features and Highlights

1. **Binary Classification**:

   * Fine-tuned BERT model labels each response as **`jailbreak`** or **`non-jailbreak`**.
   * Confidence scores accompany each prediction for thresholding and analysis.

2. **Embedding Extraction**:

   * Utilize the `[CLS]` token embedding from BERT to obtain a high-dimensional semantic representation of each response.
   * Preserve rich contextual information critical for downstream analysis.

3. **Unsupervised Clustering**:

   * **Free K-means**: Automatically determine optimal `k` via **Silhouette Score** and **Elbow Method**.
   * **Hierarchical Clustering**: Multi-level grouping to reveal subcategories within `jailbreak` and `non-jailbreak` responses.

4. **Dimensionality Reduction & Visualization**:

   * **PCA** (linear overview)
   * **t-SNE** (local structure)
   * **UMAP** (global and local balance)
   * Generate 2D plots to visually assess cluster separation and structure.

5. **Reproducibility & Portability**:

   * Fully containerized Google Colab notebooks.
   * Standardized datasets and clear folder structure.
   * Automated pipelines for data loading, model inference, clustering, and visualization.

---

## ğŸ—‚ Folder Structure Overview

```bash
LLM_Response_Classifier/
â”œâ”€â”€ CODE/
â”‚   â”œâ”€â”€ FINAL_VERSIONS/
â”‚   â”‚   â”œâ”€â”€ free_clustering/
â”‚   â”‚   â”‚   â”œâ”€â”€ Bert_tuned_cluster_e_valutazione.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ bert-no-tuned_cluster_e_valutazione.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ TestBert_fine_tuning.ipynb
â”‚   â”‚   â””â”€â”€ hierarchical_clustering/
â”‚   â”‚       â”œâ”€â”€ Bert_tuned_cluster_e_valutazione.ipynb
â”‚   â”‚       â”œâ”€â”€ bert-no-tuned_cluster_e_valutazione.ipynb
â”‚   â”‚       â””â”€â”€ TestBert_fine_tuning.ipynb
â”‚   â””â”€â”€ PREVIOUS_STEPS/
â”‚       â”œâ”€â”€ BER_Fine_Tuning_V1.ipynb
â”‚       â”œâ”€â”€ comparative_tuned_vs_nontuned/
â”‚       â”‚   â”œâ”€â”€ Bert_cluster.ipynb
â”‚       â”‚   â””â”€â”€ TestBert.ipynb
â”‚       â””â”€â”€ kmeans_k_equals_2/
â”‚           â”œâ”€â”€ Bert_tuned_2cluster.ipynb
â”‚           â”œâ”€â”€ bert-no-tuned_2cluster.ipynb
â”‚           â””â”€â”€ TestBert.ipynb
â”‚
â”œâ”€â”€ DATASETS/
â”‚   â”œâ”€â”€ fine_tuning_and_test/
â”‚   â”‚   â”œâ”€â”€ dataset_completo.json
â”‚   â”‚   â””â”€â”€ Test2.json
â”‚   â””â”€â”€ kmeans_evaluation/
â”‚       â”œâ”€â”€ balanced/
â”‚       â”‚   â”œâ”€â”€ response.json
â”‚       â”‚   â””â”€â”€ selected_responses.json
â”‚       â””â”€â”€ unbalanced/
â”‚           â”œâ”€â”€ response.json
â”‚           â””â”€â”€ response-with_label.json
â”‚
â”œâ”€â”€ reports_and_findings/
â”‚   â”œâ”€â”€ bert_tuned_vs_non_tuned/
â”‚   â”‚   â”œâ”€â”€ v1_CONFRONTO.pdf
â”‚   â”‚   â””â”€â”€ v2_REL.pdf
â”‚   â”œâ”€â”€ evaluator_reports/
â”‚   â”‚   â”œâ”€â”€ free_clustering/
â”‚   â”‚   â”‚   â”œâ”€â”€ methods.pdf
â”‚   â”‚   â”‚   â””â”€â”€ results.pdf
â”‚   â”‚   â””â”€â”€ hierarchical_clustering/
â”‚   â”‚       â”œâ”€â”€ methods.pdf
â”‚   â”‚       â””â”€â”€ results.pdf
â”‚   â”œâ”€â”€ fine_tuning_reports/
â”‚   â”‚   â”œâ”€â”€ code_evolution/
â”‚   â”‚   â”‚   â””â”€â”€ BERT_add_code_versions_1_to_5.pdf
â”‚   â”‚   â””â”€â”€ test_results/
â”‚   â”‚       â”œâ”€â”€ bert_0.0_to_1.7.pdf
â”‚   â”‚       â””â”€â”€ bert_test_1.8.pdf
â”‚   â”œâ”€â”€ vector_embeddings/
â”‚   â”‚   â”œâ”€â”€ REL.pdf
â”‚   â”‚   â””â”€â”€ rel_vettori.pdf
â”‚   â””â”€â”€ weekly_reports/
â”‚       â”œâ”€â”€ week1.pdf
â”‚       â”œâ”€â”€ week2.pdf
â”‚       â””â”€â”€ â€¦
â”‚       â””â”€â”€ week7.pdf
â”‚
â”œâ”€â”€ FINAL_REPORT/
â””â”€â”€ README.md
```

**Notes**:

* Notebooks in each subfolder are chronologically ordered.
* Clear separation between development (`PREVIOUS_STEPS`) and final deliverables (`FINAL_VERSIONS`).

---

## âš™ï¸ Installation & Setup

1. **Clone repository**:

   ```bash
   git clone https://github.com/your-username/LLM_Response_Classifier.git
   cd LLM_Response_Classifier
   ```

2. **Create virtual environment & install dependencies**:

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # Linux/macOS
   venv\Scripts\activate   # Windows
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

3. **Configure Paths** (if needed):

   * Update dataset paths in notebooks or scripts under `CODE/`.

4. **Run Notebooks**:

   * Open Google Colab and import the desired `.ipynb` file from `CODE/FINAL_VERSIONS/`.

---

## ğŸ¯ Usage Examples

### 1. Fine-Tuning BERT (if re-training)

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')

# Prepare dataset and training args...
training_args = TrainingArguments(
    output_dir='./models/bert-finetuned',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    evaluation_strategy='steps',
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)
trainer.train()
```

### 2. Embedding Extraction & Clustering

```python
from transformers import AutoModel, AutoTokenizer
from sklearn.cluster import KMeans
import torch

# Load fine-tuned model
model = AutoModel.from_pretrained('Teto03/Bert_base_fineTuned')
tokenizer = AutoTokenizer.from_pretrained('Teto03/Bert_base_fineTuned')

# Encode responses
tokenized = tokenizer(responses, padding=True, truncation=True, return_tensors='pt')
with torch.no_grad():
    outputs = model(**tokenized)
embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] embeddings

# Determine optimal k
kmeans = KMeans(n_clusters=3).fit(embeddings)
labels = kmeans.labels_
```

---

## ğŸ“ˆ Results & Insights

1. **Classification Performance**:

   * Fine-tuned BERT achieved **X% accuracy**, outperforming the base model by **Y%**.
   * Confusion matrices and ROC curves available in `reports_and_findings/bert_tuned_vs_non_tuned/`.

2. **Clustering Quality**:

   * Optimal cluster counts: `k=3` for fine-tuned, `k=2` for base model (Silhouette analysis).
   * Hierarchical clustering uncovered **Z subgroups** within jailbreak responses, suggesting thematic variations.

3. **Visualization**:

   * PCA and UMAP plots demonstrate clear separation between `jailbreak` and `non-jailbreak` clusters.
   * t-SNE highlights local structure, revealing tight clusters of similar response strategies.

---

## ğŸ“¬ Contact & Collaboration

For questions, collaborations, or feedback, please reach out:

* **Email**: [frab22207@gmail.com](mailto:frab22207@gmail.com)

