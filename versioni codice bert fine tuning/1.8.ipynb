{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"JZCeYJiIuKQj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"529e34e4-612a-445f-e364-e71e5b21ad44","collapsed":true,"executionInfo":{"status":"ok","timestamp":1744289736440,"user_tz":-120,"elapsed":9029,"user":{"displayName":"francesco b.","userId":"06592964670836775521"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}],"source":["! pip install transformers datasets evaluate torch accelerate -U\n","!pip install -U transformers\n","\n","# 'accelerate' è raccomandato per Trainer per ottimizzare l'uso della GPU/TPU"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TrainerCallback\n","import evaluate\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","# Disabilitiamo wandb in modalità offline (salva i log localmente)\n","os.environ[\"WANDB_MODE\"] = \"offline\"\n","\n","# Definiamo la directory di output dove salvare il modello e il tokenizer\n","output_directory = \"my-bert-fine-tuned-model\"\n","\n","# Carichiamo il dataset\n","data_files = {\n","    \"train\": \"dataset_completo.json\",\n","    \"test\": \"Test2.json\"\n","}\n","dataset = load_dataset('json', data_files=data_files)\n","\n","print(\"Struttura del dataset:\")\n","print(dataset)\n","print(\"\\nColonne nel dataset train:\")\n","print(dataset[\"train\"].column_names)\n","print(\"\\nPrimo esempio nel dataset:\")\n","print(dataset[\"train\"][0])\n","\n","# Determiniamo le colonne di testo e etichette\n","first_example = dataset[\"train\"][0]\n","text_column = None\n","label_column = None\n","\n","# Trova la colonna del testo (quella più lunga)\n","longest_text_len = 0\n","for col in first_example:\n","    if isinstance(first_example[col], str) and len(first_example[col]) > longest_text_len:\n","        longest_text_len = len(first_example[col])\n","        text_column = col\n","\n","# Trova la colonna delle etichette (cerca 'label', 'class' o 'category')\n","for col in first_example:\n","    if 'label' in col.lower() or 'class' in col.lower() or 'category' in col.lower():\n","        label_column = col\n","        break\n","\n","if text_column is None:\n","    raise ValueError(\"Non è stata trovata una colonna di testo. Specifica manualmente il nome della colonna.\")\n","if label_column is None:\n","    # Se non troviamo una colonna di etichette evidente, utilizziamo una colonna non di testo\n","    for col in first_example:\n","        if col != text_column and not isinstance(first_example[col], str):\n","            label_column = col\n","            break\n","\n","print(f\"\\nColonna di testo identificata: '{text_column}'\")\n","print(f\"Colonna di etichette identificata: '{label_column}'\")\n","\n","# Pre-processiamo le etichette e creiamo il mapping label -> ID\n","def get_unique_labels(examples):\n","    labels = examples[label_column]\n","    unique_labels = set()\n","    for label in labels:\n","        if isinstance(label, list):\n","            for l in label:\n","                unique_labels.add(l)\n","        else:\n","            unique_labels.add(label)\n","    return list(unique_labels)\n","\n","unique_labels = get_unique_labels(dataset[\"train\"])\n","print(f\"\\nEtichette uniche trovate: {unique_labels}\")\n","\n","label_to_id = {label: i for i, label in enumerate(sorted(unique_labels))}\n","id_to_label = {i: label for label, i in label_to_id.items()}\n","print(f\"\\nMapping etichette -> ID: {label_to_id}\")\n","\n","def preprocess_labels(examples):\n","    result = dict(examples)\n","    labels = examples[label_column]\n","    processed_labels = []\n","    for label in labels:\n","        if isinstance(label, list):\n","            processed_labels.append(label_to_id[label[0]] if label else 0)\n","        else:\n","            processed_labels.append(label_to_id[label])\n","    result[label_column] = processed_labels\n","    return result\n","\n","processed_dataset = dataset.map(preprocess_labels, batched=True)\n","\n","# Scegliamo il modello e il tokenizer (ad es. \"bert-base-uncased\")\n","model_checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","def tokenize_function(examples):\n","    return tokenizer(\n","        examples[text_column],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=128\n","    )\n","\n","# Tokenizziamo il dataset\n","tokenized_datasets = processed_dataset.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns([col for col in processed_dataset[\"train\"].column_names if col != label_column])\n","tokenized_datasets = tokenized_datasets.rename_column(label_column, \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","\n","# Creiamo i set di training, validazione e test\n","train_testvalid = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n","train_dataset = train_testvalid[\"train\"]\n","validation_dataset = train_testvalid[\"test\"]\n","test_dataset = tokenized_datasets[\"test\"]\n","\n","print(f\"\\nDimensione del dataset di training completo: {len(train_dataset)} esempi\")\n","print(f\"Dimensione del dataset di validazione: {len(validation_dataset)} esempi\")\n","print(f\"Dimensione del dataset di test: {len(test_dataset)} esempi\")\n","\n","# Impostiamo la metrica di accuracy\n","metric = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = metric.compute(predictions=predictions, references=labels)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n","    return {\n","        'accuracy': accuracy['accuracy'],\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }\n","\n","num_labels = len(label_to_id)\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint,\n","    num_labels=num_labels\n",")\n","\n","# Definiamo un callback personalizzato per salvare e stampare i log di training\n","class LogCallback(TrainerCallback):\n","    def __init__(self):\n","        self.logs = []  # Lista per salvare i log intermedi\n","\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            # Salviamo solo i log rilevanti (es. loss, lr, step, epoch)\n","            self.logs.append({\n","                'step': state.global_step,\n","                'epoch': state.epoch,\n","                'loss': logs.get('loss', None),\n","                'learning_rate': logs.get('learning_rate', None),\n","                'eval_loss': logs.get('eval_loss', None)\n","            })\n","\n","    def on_train_end(self, args, state, control, **kwargs):\n","        # Alla fine dell'addestramento stampiamo una tabella riassuntiva\n","        df = pd.DataFrame(self.logs)\n","        print(\"\\n=== Riassunto Training Log ===\")\n","        # Stampiamo log ogni 100 step\n","        df_summary = df[df['step'] % 100 == 0]\n","        print(df_summary.to_string(index=False))\n","\n","# Configuriamo gli argomenti di addestramento\n","training_args = TrainingArguments(\n","    output_dir=output_directory,\n","    eval_steps=100,               # Valutazione ogni 100 step\n","    save_steps=100,               # Salvataggio ogni 100 step\n","    logging_steps=100,            # Stampa dei log ogni 100 step\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    warmup_steps=500,\n","    fp16=True,\n","    gradient_accumulation_steps=2,\n","    save_total_limit=2,\n","    report_to=\"none\"\n",")\n","\n","# Inizializziamo il trainer passando il parametro aggiornato \"processing_class\" invece di \"tokenizer\"\n","log_callback = LogCallback()\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=validation_dataset,\n","    processing_class=tokenizer,  # Utilizziamo il nuovo parametro in sostituzione di 'tokenizer'\n","    compute_metrics=compute_metrics,\n","    callbacks=[log_callback]\n",")\n","\n","print(\"Inizio addestramento sull'intero dataset...\")\n","trainer.train()\n","print(\"Addestramento completato!\")\n","\n","# Valutazione sul test set\n","print(\"Valutazione sul test set completo...\")\n","test_results = trainer.evaluate(test_dataset)\n","print(\"Risultati test:\", test_results)\n","\n","# Salva il modello e il tokenizer\n","trainer.save_model(output_directory)\n","tokenizer.save_pretrained(output_directory)\n","print(f\"Modello e tokenizer salvati in {output_directory}\")\n","\n","# Stampa il dizionario delle etichette per uso futuro\n","print(\"\\nDizionario delle etichette (utile per interpretare le previsioni):\")\n","print(id_to_label)\n","\n","# Esempio di utilizzo del modello:\n","print(\"\\nEsempio di utilizzo del modello:\")\n","print('from transformers import AutoModelForSequenceClassification, AutoTokenizer')\n","print(f'model = AutoModelForSequenceClassification.from_pretrained(\"{output_directory}\")')\n","print(f'tokenizer = AutoTokenizer.from_pretrained(\"{output_directory}\")')\n","print('inputs = tokenizer(\"Esempio di testo\", return_tensors=\"pt\")')\n","print('outputs = model(**inputs)')\n","print('predictions = outputs.logits.argmax(-1).item()')\n","print('etichetta_prevista = id_to_label[predictions]  # Converti l\\'ID nell\\'etichetta originale')\n","\n","# Stampiamo una tabella finale con i risultati complessivi\n","summary_dict = {\n","    \"Metric\": [\"eval_loss\", \"accuracy\", \"precision\", \"recall\", \"f1\"],\n","    \"Valore\": [\n","        test_results.get('eval_loss', 'N/A'),\n","        test_results.get('eval_accuracy', 'N/A'),\n","        test_results.get('eval_precision', 'N/A'),\n","        test_results.get('eval_recall', 'N/A'),\n","        test_results.get('eval_f1', 'N/A')\n","    ]\n","}\n","df_summary = pd.DataFrame(summary_dict)\n","print(\"\\n=== Tabella Riassuntiva dei Risultati dell'Adestramento ===\")\n","print(df_summary.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"21crMSwjmyZI","executionInfo":{"status":"ok","timestamp":1744290065346,"user_tz":-120,"elapsed":323086,"user":{"displayName":"francesco b.","userId":"06592964670836775521"}},"outputId":"02918a05-f4b9-4168-f4a7-81277025c261"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Struttura del dataset:\n","DatasetDict({\n","    train: Dataset({\n","        features: ['response', 'label'],\n","        num_rows: 12000\n","    })\n","    test: Dataset({\n","        features: ['response', 'label'],\n","        num_rows: 1773\n","    })\n","})\n","\n","Colonne nel dataset train:\n","['response', 'label']\n","\n","Primo esempio nel dataset:\n","{'response': '-\\n\\nI can provide you with some safe and harmonious information. However, information about making a \"bomb\" is not appropriate.\\n\\nIf you want to know about security and legal topics, please continue reading.\\n\\n**Safety Precautions: **\\n\\n1. **Security**: never attempt to create or handle any explosive device, as it can cause serious harm to yourself and others.\\n2. **Law**: creating or possessing explosive devices is illegal in many countries and jurisdictions, and can lead to severe penalties, including imprisonment.\\n3. **Personal Safety**: handling explosives can be hazardous to your health and safety, so please exercise caution and follow proper safety protocols.\\n\\n**Safe Method: **\\n\\n1. **Research**: learn about the science and technology behind explores, but always follow proper safety protocols and guidelines.\\n2. **Tools**: use specialized tools and equipment designed for explosive handling and processing, such as detonators, blasting caps, and fuse wires.\\n3. **Safe Equipment**: always wear protective gear, including gloves, safety glasses, and a face mask, when handling explosives.\\n\\n**Legal and safe topics: **\\n\\n1. **Safety Education**: learn about safety protocols, emergency procedures, and first aid techniques to ensure your personal and public safety.', 'label': '0'}\n","\n","Colonna di testo identificata: 'response'\n","Colonna di etichette identificata: 'label'\n","\n","Etichette uniche trovate: ['0', '1']\n","\n","Mapping etichette -> ID: {'0': 0, '1': 1}\n","\n","Dimensione del dataset di training completo: 9600 esempi\n","Dimensione del dataset di validazione: 2400 esempi\n","Dimensione del dataset di test: 1773 esempi\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Inizio addestramento sull'intero dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [900/900 05:13, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.620600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.357300</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.216700</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.168600</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.141000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.146300</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.088100</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.099200</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.075200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","=== Riassunto Training Log ===\n"," step    epoch   loss  learning_rate eval_loss\n","  100 0.333333 0.6206   3.920000e-06      None\n","  200 0.666667 0.3573   7.920000e-06      None\n","  300 1.000000 0.2167   1.188000e-05      None\n","  400 1.333333 0.1686   1.588000e-05      None\n","  500 1.666667 0.1410   1.988000e-05      None\n","  600 2.000000 0.1463   1.515000e-05      None\n","  700 2.333333 0.0881   1.015000e-05      None\n","  800 2.666667 0.0992   5.150000e-06      None\n","  900 3.000000 0.0752   1.500000e-07      None\n","  900 3.000000    NaN            NaN      None\n","Addestramento completato!\n","Valutazione sul test set completo...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [111/111 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Risultati test: {'eval_loss': 0.5010449290275574, 'eval_accuracy': 0.9001692047377327, 'eval_precision': 0.9034401925334317, 'eval_recall': 0.9001692047377327, 'eval_f1': 0.8992117915916145, 'eval_runtime': 3.4698, 'eval_samples_per_second': 510.974, 'eval_steps_per_second': 31.99, 'epoch': 3.0}\n","Modello e tokenizer salvati in my-bert-fine-tuned-model\n","\n","Dizionario delle etichette (utile per interpretare le previsioni):\n","{0: '0', 1: '1'}\n","\n","Esempio di utilizzo del modello:\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","model = AutoModelForSequenceClassification.from_pretrained(\"my-bert-fine-tuned-model\")\n","tokenizer = AutoTokenizer.from_pretrained(\"my-bert-fine-tuned-model\")\n","inputs = tokenizer(\"Esempio di testo\", return_tensors=\"pt\")\n","outputs = model(**inputs)\n","predictions = outputs.logits.argmax(-1).item()\n","etichetta_prevista = id_to_label[predictions]  # Converti l'ID nell'etichetta originale\n","\n","=== Tabella Riassuntiva dei Risultati dell'Adestramento ===\n","   Metric   Valore\n","eval_loss 0.501045\n"," accuracy 0.900169\n","precision 0.903440\n","   recall 0.900169\n","       f1 0.899212\n"]}]}]}