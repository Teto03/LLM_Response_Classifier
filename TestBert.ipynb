{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teto03/Tirocinio/blob/main/TestBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JZCeYJiIuKQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e62c06-ae46-447d-fefc-c1ddfac0b00d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 174, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 162, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 109, in _iter_built_with_inserted\n",
            "    for version, func in infos:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 300, in iter_index_candidate_infos\n",
            "    result = self._finder.find_best_candidate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/package_finder.py\", line 884, in find_best_candidate\n",
            "    candidates = self.find_all_candidates(project_name)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/package_finder.py\", line 825, in find_all_candidates\n",
            "    page_candidates = list(page_candidates_it)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/sources.py\", line 194, in page_candidates\n",
            "    yield from self._candidates_from_page(self._link)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/package_finder.py\", line 785, in process_project_url\n",
            "    index_response = self._link_collector.fetch_response(project_url)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/collector.py\", line 448, in fetch_response\n",
            "    return _get_index_content(location, session=self.session)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/collector.py\", line 328, in _get_index_content\n",
            "    vcs_scheme = _match_vcs_scheme(url)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/collector.py\", line 57, in _match_vcs_scheme\n",
            "    if url.lower().startswith(scheme) and url[len(scheme)] in \"+:\":\n",
            "       ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers datasets evaluate torch accelerate -U\n",
        "# 'accelerate' è raccomandato per Trainer per ottimizzare l'uso della GPU/TPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disabilitiamo wandb, ma solo localmente in questa esecuzione (ormai hai creato l'account)\n",
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"  # Questa opzione funziona offline ma salva i log localmente\n",
        "\n",
        "# Reimportiamo le librerie necessarie\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "\n",
        "# Carichiamo nuovamente il dataset\n",
        "data_files = {\n",
        "    \"train\": \"translated_merged_responses.json\",\n",
        "    \"test\": \"test.json\"\n",
        "}\n",
        "\n",
        "dataset = load_dataset('json', data_files=data_files)\n",
        "\n",
        "# Analizziamo brevemente il dataset\n",
        "print(\"Struttura del dataset:\")\n",
        "print(dataset)\n",
        "print(\"\\nColonne nel dataset train:\")\n",
        "print(dataset[\"train\"].column_names)\n",
        "print(\"\\nPrimo esempio nel dataset:\")\n",
        "print(dataset[\"train\"][0])\n",
        "\n",
        "# Determiniamo i nomi corretti delle colonne\n",
        "# Questo codice cercherà automaticamente le colonne che contengono testo e etichette\n",
        "first_example = dataset[\"train\"][0]\n",
        "text_column = None\n",
        "label_column = None\n",
        "\n",
        "# Trova la colonna del testo (la colonna con stringhe più lunghe)\n",
        "longest_text_len = 0\n",
        "for col in first_example:\n",
        "    if isinstance(first_example[col], str) and len(first_example[col]) > longest_text_len:\n",
        "        longest_text_len = len(first_example[col])\n",
        "        text_column = col\n",
        "\n",
        "# Trova la colonna delle etichette (cerca 'label' o simili)\n",
        "for col in first_example:\n",
        "    if 'label' in col.lower() or 'class' in col.lower() or 'category' in col.lower():\n",
        "        label_column = col\n",
        "        break\n",
        "\n",
        "if text_column is None:\n",
        "    raise ValueError(\"Non è stata trovata una colonna di testo. Specifica manualmente il nome della colonna.\")\n",
        "\n",
        "if label_column is None:\n",
        "    # Se non troviamo una colonna di etichette evidente, utilizziamo una colonna non di testo\n",
        "    for col in first_example:\n",
        "        if col != text_column and not isinstance(first_example[col], str):\n",
        "            label_column = col\n",
        "            break\n",
        "\n",
        "print(f\"\\nColonna di testo identificata: '{text_column}'\")\n",
        "print(f\"Colonna di etichette identificata: '{label_column}'\")\n",
        "\n",
        "# Verifichiamo il tipo delle etichette e le trasformiamo in numeri interi\n",
        "def get_unique_labels(examples):\n",
        "    labels = examples[label_column]\n",
        "    unique_labels = set()\n",
        "    for label in labels:\n",
        "        if isinstance(label, list):\n",
        "            for l in label:\n",
        "                unique_labels.add(l)\n",
        "        else:\n",
        "            unique_labels.add(label)\n",
        "    return list(unique_labels)\n",
        "\n",
        "unique_labels = get_unique_labels(dataset[\"train\"])\n",
        "print(f\"\\nEtichette uniche trovate: {unique_labels}\")\n",
        "\n",
        "# Creiamo un mapping delle etichette agli ID\n",
        "label_to_id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
        "id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "\n",
        "print(f\"\\nMapping etichette -> ID: {label_to_id}\")\n",
        "\n",
        "# Funzione per preprocessare le etichette\n",
        "def preprocess_labels(examples):\n",
        "    # Copia gli esempi per non modificare il dataset originale\n",
        "    result = dict(examples)\n",
        "\n",
        "    # Converti le etichette in interi\n",
        "    labels = examples[label_column]\n",
        "    processed_labels = []\n",
        "\n",
        "    for label in labels:\n",
        "        if isinstance(label, list):\n",
        "            # Se l'etichetta è una lista, prendi il primo elemento\n",
        "            if label:\n",
        "                processed_labels.append(label_to_id[label[0]])\n",
        "            else:\n",
        "                processed_labels.append(0)  # Valore di default\n",
        "        else:\n",
        "            # Se l'etichetta è un singolo valore\n",
        "            processed_labels.append(label_to_id[label])\n",
        "\n",
        "    # Sostituisci la colonna originale con le etichette processate\n",
        "    result[label_column] = processed_labels\n",
        "    return result\n",
        "\n",
        "# Applica il preprocessing alle etichette\n",
        "processed_dataset = dataset.map(preprocess_labels, batched=True)\n",
        "\n",
        "# Scegliamo il modello e il tokenizer\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Funzione per tokenizzare il testo\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[text_column],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128  # Imposta una lunghezza appropriata\n",
        "    )\n",
        "\n",
        "# Applica la tokenizzazione\n",
        "tokenized_datasets = processed_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Prepara il dataset per PyTorch\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([col for col in processed_dataset[\"train\"].column_names if col != label_column])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(label_column, \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# Crea un validation set separato dal train set\n",
        "train_testvalid = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_testvalid[\"train\"]\n",
        "validation_dataset = train_testvalid[\"test\"]\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "# Crea subset piccoli per test (opzionale)\n",
        "small_train_dataset = train_dataset.shuffle(seed=42).select(range(min(1000, len(train_dataset))))\n",
        "small_eval_dataset = validation_dataset.shuffle(seed=42).select(range(min(200, len(validation_dataset))))\n",
        "\n",
        "# Imposta le metriche\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Aggiungi più metriche se necessario\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    metrics = {\n",
        "        'accuracy': accuracy['accuracy'],\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Carica il modello con il numero corretto di etichette\n",
        "num_labels = len(label_to_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Configura l'addestramento\n",
        "output_directory = \"my-bert-fine-tuned-model\"\n",
        "\n",
        "# Imposta gli argomenti di addestramento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"  # Disabilita completamente wandb e altri logger\n",
        ")\n",
        "\n",
        "# Inizializza il Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,  # Usa train_dataset per l'addestramento completo\n",
        "    eval_dataset=small_eval_dataset,    # Usa validation_dataset per l'addestramento completo\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Addestra il modello\n",
        "print(\"Inizio addestramento...\")\n",
        "trainer.train()\n",
        "print(\"Addestramento completato!\")\n",
        "\n",
        "# Valutazione sul test set\n",
        "print(\"Valutazione sul test set...\")\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Risultati test:\", test_results)\n",
        "\n",
        "# Salva il modello\n",
        "trainer.save_model(output_directory)\n",
        "tokenizer.save_pretrained(output_directory)\n",
        "print(f\"Modello e tokenizer salvati in {output_directory}\")\n",
        "\n",
        "# Stampa il dizionario delle etichette per utilizzo futuro\n",
        "print(\"\\nDizionario delle etichette (utile per interpretare le previsioni):\")\n",
        "print(id_to_label)\n",
        "\n",
        "# Esempio di come utilizzare il modello:\n",
        "print(\"\\nEsempio di utilizzo del modello:\")\n",
        "print(\"from transformers import AutoModelForSequenceClassification, AutoTokenizer\")\n",
        "print(\"model = AutoModelForSequenceClassification.from_pretrained(\\\"\" + output_directory + \"\\\")\")\n",
        "print(\"tokenizer = AutoTokenizer.from_pretrained(\\\"\" + output_directory + \"\\\")\")\n",
        "print(\"inputs = tokenizer(\\\"Esempio di testo\\\", return_tensors=\\\"pt\\\")\")\n",
        "print(\"outputs = model(**inputs)\")\n",
        "print(\"predictions = outputs.logits.argmax(-1).item()\")\n",
        "print(\"etichetta_prevista = id_to_label[predictions]  # Converti l'ID nell'etichetta originale\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "21crMSwjmyZI",
        "outputId": "de26b74b-4499-4062-891b-38cc48770ef6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3901e63f9ff8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Reimportiamo le librerie necessarie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}