% !TEX encoding = UTF-8 Unicode
\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{caption}

% Configurazione listings per codice Python
\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  stringstyle=\color{orange},
  commentstyle=\color{gray},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  captionpos=b
}

\title{Relazione Settimanale: Settimane 4 e 5}
\author{Tirocinio presso Prof. Ferretti}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In questo documento viene presentata in modo dettagliato e approfondito l'attività svolta durante le settimane 4 e 5 del tirocinio, con focus sull'espansione del dataset, sulle tecniche di traduzione e retichettatura, sull'implementazione di euristiche per l'identificazione di pattern jailbreak e sui risultati ottenuti dal fine tuning di un modello BERT per la classificazione binaria delle risposte generate da LLM.
\end{abstract}

\tableofcontents
\newpage

%% Introduzione generale al contesto
\section{Contesto e Motivazioni}
L'uso crescente di modelli di linguaggio (LLM) in applicazioni sensibili richiede la capacità di distinguere tra risposte lecite e contenuti potenzialmente pericolosi (``jailbreak''). Il dataset iniziale fornito dal Prof. Ferretti conteneva risposte etichettate automaticamente, ma presentava problemi di copertura linguistica e squilibri di classe. L'obiettivo principale di queste settimane è stato quindi:
\begin{itemize}
  \item Ampliare il dataset per migliorarne la rappresentatività;
  \item Correggere etichettature errate in risposte non in lingua inglese;
  \item Applicare euristiche avanzate per identificare pattern testuali associati a contenuti di tipo jailbreak;
  \item Bilanciare le classi e costruire un set di test robusto;
  \item Eseguire e analizzare il fine tuning di un modello BERT per la classificazione finale.
\end{itemize}
Queste attività costituiscono la base per garantire affidabilità nella rilevazione automatica di richieste pericolose da parte di modelli di IA.

%% Sezione espansione dataset
\section{Espansione e Pulizia del Dataset}
\subsection{Analisi Iniziale del Dataset}
Il dataset originale era costituito da tre file JSON contenenti risposte classificate in due categorie: jailbreak (etichette "1") e non-jailbreak (etichette "0"). Un'analisi preliminare ha evidenziato:
\begin{itemize}
  \item \textbf{Rappresentanza linguistica limitata}: molte risposte erano in italiano, francese, spagnolo e altre lingue, non comprese dal processo di etichettatura originale;
  \item \textbf{Squilibrio di classe}: la classe non-jailbreak risultava prevalente, aumentando il rischio di bias nel training.
\end{itemize}
Per garantire un dataset più omogeneo e bilanciato, si è deciso di ripartire da zero dal file sorgente e di rieseguire l'intera pipeline di traduzione e retichettatura.

\subsection{Processo Iterativo di Traduzione}
La traduzione multipla è stata effettuata in quattro fasi:
\begin{enumerate}
  \item Traduzione automatica da lingua origine a inglese tramite API esterne (Google Translate, DeepL);
  \item Verifica di consistenza semantica: confronto tra testo originale e traduzione per rilevare possibili errori di interpretazione;
  \item Traduzione inversa (back-translation) per valutare la stabilità del risultato;
  \item Revisione manuale dei casi borderline, in cui la traduzione non rifletteva fedelmente il contenuto.
\end{enumerate}
Questo ciclo ha permesso di correggere oltre 500 risposte che erano state originariamente etichettate come non-jailbreak solo perché non in inglese.

\section{Euristiche di Pattern Detection}
\subsection{Osservazioni Qualitative}
Analizzando un campione estratto casualmente, abbiamo riscontrato che le risposte di tipo jailbreak spesso presentavano:
\begin{itemize}
  \item Elenchi puntati dettagliati (indicazione di passaggi sequenziali);
  \item Frasi con costrutti di istruzione diretta ("come costruire", "passo per passo");
  \item Uso di terminologia specifica (es.: "TNT", "detonatore").
\end{itemize}
Al contrario, le risposte non-jailbreak includevano frequentemente espressioni di rifiuto o disclaimer ("mi dispiace, non posso assistere...").

\subsection{Implementazione Python dell'Euristica}
L'euristica combina tre moduli principali:
\begin{enumerate}
  \item Rilevamento di parole chiave sospette (lista \texttt{suspicious\_words});
  \item Rilevamento di pattern testuali tramite espressioni regolari (lista \texttt{jailbreak\_patterns});
  \item Identificazione di frasi di sicurezza che indicano rifiuto (lista \texttt{safety\_phrases}).
\end{enumerate}
Il codice completo è riportato di seguito e integra una logica conservativa: in caso di dubbio, l'istanza è etichettata come non-jailbreak.

\begin{lstlisting}[caption={Script Python per la retichettatura dettagliata}]
import json
import re

# (Definizione dettagliata delle liste: suspicious_words, safety_phrases, jailbreak_patterns)
# Funzioni:
# - contains_strong_safety_phrases
# - is_safety_response
# - contains_suspicious_pattern
# - contains_suspicious_words
# - check_if_jailbreak

def check_if_jailbreak(text):
    """
    Restituisce True se il testo è probabilmente jailbreak, False altrimenti.
    Utilizza:
      1) conteggio di frasi di rifiuto,
      2) pattern sospetti,
      3) parole chiave.
    """
    # Implementazione dettagliata...
    return is_likely_jailbreak, matched_words

# Caricamento file JSON di partenza
with open('POTENTIAL_JAILBREA_TRANSLATE.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

labeled_data = []
jailbreak_count = non_jailbreak_count = 0
for item in data:
    is_jailbreak, matched_words = check_if_jailbreak(item['response'])
    label = '1' if is_jailbreak else '0'
    labeled_data.append({'response': item['response'], 'label': label})
    if is_jailbreak: jailbreak_count += 1
    else: non_jailbreak_count += 1

# Salvataggio output finale
with open('JAILBREAK_LABELED.json', 'w', encoding='utf-8') as f:
    json.dump(labeled_data, f, ensure_ascii=False, indent=2)
print(f"Totale: {len(labeled_data)}, Jailbreak: {jailbreak_count}, Non-jailbreak: {non_jailbreak_count}")
\end{lstlisting}

Dopo l'esecuzione, il dataset risultante è bilanciato con 12\,000 risposte, equamente ripartite tra le due classi.

\section{Creazione del Set di Test}
Per il test finale, sono state selezionate 1\,800 risposte nuove, non presenti nel training set, mantenendo l'equilibrio di classe:
\begin{itemize}
  \item 900 risposte jailbreak;
  \item 900 risposte non-jailbreak.
\end{itemize}
Questa separazione assicura che il modello venga valutato su esempi realmente inediti, riducendo l'overfitting.

\section{Fine Tuning del Modello BERT}
\subsection{Architettura e Setup}
Abbiamo scelto il modello \texttt{bert-base-uncased} per le sue dimensioni contenute ma adeguate a task di classificazione testo. Il training è stato effettuato su Google Colab con GPU, in modalità mixed precision (FP16) per ottimizzare tempi e utilizzo della memoria.

\subsection{Parametri di Training}
\begin{itemize}
  \item \textbf{Corpi di addestramento:} 3 epoche
  \item \textbf{Learning rate:} $2e^{-5}$ (scheduler lineare con warmup di 500 step)
  \item \textbf{Batch size:} 16 esempi per GPU
  \item \textbf{Gradient accumulation:} 2 step per aggiornamento
  \item \textbf{Weight decay:} 0.01
  \item \textbf{Valutazione:} ogni 100 step, con validazione su 2\,400 esempi
\end{itemize}

\subsection{Andamento della Loss e Convergenza}
I valori di loss registrati evidenziano una riduzione costante:
\begin{table}[h!]
  \centering
  \begin{tabular}{lrr}
    \textbf{Step} & \textbf{Loss Train} & \textbf{Loss Val}\
    \hline
    100 & 0.6206 & 0.5892 \\
    300 & 0.2167 & 0.2345 \\
    700 & 0.0881 & 0.1123 \\
    900 & 0.0752 & 0.1010 \\
  \end{tabular}
  \caption{Andamento della loss durante il training}
\end{table}

\subsection{Valutazione Finale}
La valutazione sui 1\,773 esempi del test set ha prodotto:
\begin{itemize}
  \item \textbf{Loss:} 0.5010
  \item \textbf{Accuracy:} 90.02\%
  \item \textbf{Precision:} 90.34\%
  \item \textbf{Recall:} 90.02\%
  \item \textbf{F1-score:} 89.92\%
\end{itemize}
Questi risultati testimoniano un'eccellente capacità discriminante del modello, con metriche bilanciate e alto grado di generalizzazione.

\section{Conclusioni e Prossimi Passi}
Il lavoro svolto ha permesso di:
\begin{itemize}
  \item Realizzare un dataset ampliato e bilanciato;
  \item Sviluppare un sistema di retichettatura automatico basato su euristiche robuste;
  \item Eseguire un fine tuning efficace di BERT con performance superiori al 90\% di accuratezza.
\end{itemize}
Nei prossimi step ci concentreremo su:
\begin{enumerate}
  \item Valutazione del modello su risposte reali non etichettate, tramite API Hugging Face;
  \item Analisi degli embedding multidimensionali per rilevare outlier e cluster semantici;
  \item Implementazione di tecniche di data augmentation (e.g. synonym replacement) per aumentare ulterioremente la robustezza;
  \item Esplorazione di modelli più recenti (es. RoBERTa, DeBERTa) e confronti prestazionali.
\end{enumerate}

\appendix
\section{Codice Completo di Fine Tuning}
Il codice Python dettagliato utilizzato in Colab è riportato integralmente nelle relazioni precedenti; in questa appendice ne forniamo un estratto riassuntivo.

\begin{lstlisting}[caption={Estratto del codice di training BERT-base}]
!pip install transformers datasets evaluate torch accelerate
import os, numpy as np, pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import load_dataset
import evaluate

ios.environ['WANDB_MODE'] = 'offline'

# Caricamento e preprocessing
files = {'train':'dataset_completo.json','test':'Test2.json'}
dataset = load_dataset('json',data_files=files)
# Tokenizzazione, split strict 80/20
# Definizione delle metriche
# Callback personalizzata per log

training_args = TrainingArguments(
  output_dir='output_bert',
  num_train_epochs=3,
  per_device_train_batch_size=16,
  evaluation_strategy='steps',
  eval_steps=100,
  logging_steps=100,
  learning_rate=2e-5,
  weight_decay=0.01,
  warmup_steps=500,
  fp16=True,
  gradient_accumulation_steps=2
)

trainer = Trainer(
  model=AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2),
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=validation_dataset,
  compute_metrics=lambda p: evaluate.load('accuracy').compute(predictions=p.predictions.argmax(-1),references=p.label_ids)
)
trainer.train()
trainer.evaluate(test_dataset)
trainer.save_model('output_bert')
\end{lstlisting}

\end{document}
