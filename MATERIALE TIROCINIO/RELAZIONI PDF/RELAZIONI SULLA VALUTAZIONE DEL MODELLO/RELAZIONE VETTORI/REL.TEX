% Relazione LaTeX: Estrazione degli embedding da BERT e clustering con K-Means
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{listings}
\usepackage{xcolor}

% Impostazioni per codice
\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  frame=single,
  breaklines=true,
}

\title{Estrazione degli Embedding da BERT e Clustering con K-Means}
\author{Autore: \textit{Tuo Nome}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In questa relazione descriviamo in dettaglio il processo di estrazione dei vettori (embedding) dal modello BERT \texttt{\detokenize{Teto03/Bert_base_fineTuned}}, la loro struttura e dimensione, e come vengono utilizzati per il clustering mediante l'algoritmo K-Means. 
\end{abstract}

\section{Introduzione}
I modelli di linguaggio basati su Transformer, come BERT (Bidirectional Encoder Representations from Transformers), rappresentano ogni input testuale tramite vettori numerici ad alta dimensionalità. Questi vettori, detti \emph{embedding}, contengono informazioni semantiche utili per compiti di classificazione, clustering e molti altri.

\section{Tokenizzazione e Input di BERT}
Il testo da analizzare viene innanzitutto tokenizzato utilizzando il tokenizer associato al modello. Per ogni sequenza di input:
\begin{enumerate}
  \item Il tokenizer suddivide il testo in \emph{subword tokens}. 
  \item Vengono aggiunti i token speciali:
    \begin{itemize}
      \item \texttt{[CLS]} all'inizio della sequenza,
      \item \texttt{[SEP]} alla fine.
    \end{itemize}
  \item L'input viene convertito in ID di token e maschere di attenzione.
\end{enumerate}

\section{Estrazione degli Hidden States}
Chiamando il modello con l'opzione \texttt{output\_hidden\_states=True}, otteniamo:
\[
\texttt{outputs.hidden\_states} = \{H^0, H^1, \dots, H^L\},
\]
dove \( L \) è il numero di layer (per \texttt{bert-base}, \( L = 12 \)), e ciascun \( H^l \in \mathbb{R}^{\mathrm{batch} \times \mathrm{seq\_len} \times d} \), con \( d = 768 \).

\section{Vettore [CLS] come Embedding della Frase}
Per rappresentare l'intera sequenza, estraiamo il vettore corrispondente al token \texttt{[CLS]} dall'ultimo layer $H^L$:
\[
\mathbf{e} = H^L[:, 0, :] \in \mathbb{R}^{d} = \mathbb{R}^{768}.
\]
Qui la dimensione $768$ è specifica del modello \texttt{bert-base}. Il vettore $\mathbf{e}$ contiene una rappresentazione compatta dell'intero testo.

\section{Costruzione della Matrice di Embedding}
Ripetendo il processo per ciascuna delle $N$ risposte caricate da \texttt{response.json}, otteniamo un insieme di vettori:
\[
\{\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_N\},\quad \mathbf{e}_i \in \mathbb{R}^{768}.
\]
Concatenando verticalmente, si forma la matrice:
\[
X = \begin{bmatrix}
--- \\
\mathbf{e}_1^T \\
--- \\
\mathbf{e}_2^T \\
\vdots \\
\mathbf{e}_N^T \\
---
\end{bmatrix} \in \mathbb{R}^{N \times 768}.
\]

\section{Clustering con K-Means}
L'algoritmo K-Means richiede in input la matrice $X$. Fissato il numero di cluster $K$ (es. $K=2$), si procede:
\begin{enumerate}
  \item Inizializzazione casuale dei centroidi $\{\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_K\}$ in $\mathbb{R}^{768}$.
  \item Iterazioni:
    \begin{itemize}
      \item Assegna ciascun vettore $\mathbf{e}_i$ al cluster di cui il centroide è più vicino (distanza Euclidea):
      \[
      c(i) = \arg\min_{j \in \{1,\dots,K\}} \|\mathbf{e}_i - \boldsymbol{\mu}_j\|_2.
      \]
      \item Aggiorna i centroidi come media dei vettori assegnati:
      \[
      \boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{i: c(i)=j} \mathbf{e}_i.
      \]
    \end{itemize}
  \item Convergenza quando le assegnazioni non cambiano più.
\end{enumerate}

\section{Riduzione Dimensionale per Visualizzazione}
Per rappresentare i dati in 2D:
\begin{itemize}
  \item \textbf{PCA}: proiezione lineare da $\mathbb{R}^{768}$ a $\mathbb{R}^2$ mantenendo la massima varianza.
  \item \textbf{t-SNE}: mappa non lineare che preserva le distanze locali in uno spazio bidimensionale.
\end{itemize}
Le figure risultanti sono salvate come \texttt{clustering\_pca.png} e \texttt{clustering\_tsne.png}.

\section{Conclusioni}
In questa relazione abbiamo mostrato passo dopo passo come le risposte generate da BERT vengano convertite in vettori densi di dimensione $768$ e come questi vengano successivamente soggetti a clustering con K-Means. Le tecniche di riduzione dimensionale permettono una visualizzazione intuitiva dei risultati.

\end{document}
