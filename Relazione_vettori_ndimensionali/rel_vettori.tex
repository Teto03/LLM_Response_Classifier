\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codepurple},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegreen},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{Gestione ed Analisi dei Vettori di Embedding prodotti da un Modello BERT Affinato}
\author{Analisi Tecnica}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Questo documento analizza la metodologia di estrazione e gestione dei vettori di embedding generati da un modello BERT affinato, caricato da Hugging Face. L'analisi comprende l'elaborazione delle rappresentazioni vettoriali delle risposte, le tecniche di clustering applicate e l'interpretazione dei risultati ottenuti attraverso visualizzazioni bidimensionali utilizzando metodi di riduzione dimensionale come PCA e t-SNE. Lo studio evidenzia l'efficacia del modello "Bert\_base\_fineTuned" nell'organizzare semanticamente le risposte e identifica pattern linguistici emergenti dai cluster formati.
\end{abstract}

\tableofcontents

\newpage

\section{Introduzione}

I modelli di linguaggio basati su transformer come BERT (Bidirectional Encoder Representations from Transformers) hanno rivoluzionato l'elaborazione del linguaggio naturale grazie alla loro capacità di generare rappresentazioni vettoriali contestuali. Questo studio analizza l'applicazione di un modello BERT affinato (fine-tuned) su compiti specifici, focalizzandosi sulla metodologia di estrazione degli embedding e sul loro utilizzo per la segmentazione semantica delle risposte.

Il modello specifico utilizzato, "Teto03/Bert\_base\_fineTuned", è stato caricato attraverso la libreria Hugging Face Transformers, che fornisce un'interfaccia standardizzata per l'accesso a modelli pre-addestrati e affinati. L'obiettivo principale è quello di analizzare come queste rappresentazioni vettoriali catturino le caratteristiche semantiche delle risposte e come possano essere utilizzate per identificare pattern linguistici attraverso tecniche di clustering.

\section{Fondamenti Teorici}

\subsection{Modelli BERT e Rappresentazioni Vettoriali}

BERT è un modello transformer bidirezionale che genera rappresentazioni contestuali di ogni token in un testo. A differenza dei modelli tradizionali word embedding come Word2Vec o GloVe, BERT considera il contesto completo in cui una parola appare, generando embedding diversi per la stessa parola in contesti differenti.

La struttura di BERT comprende:

\begin{itemize}
    \item \textbf{Layer di input}: tokenizzazione del testo e trasformazione in ID numerici
    \item \textbf{Layer di embedding}: conversione degli ID in vettori iniziali
    \item \textbf{Encoder Transformer}: elaborazione contestuale attraverso meccanismi di self-attention
    \item \textbf{Layer di output}: generazione dei vettori finali per ciascun token
\end{itemize}

L'innovazione di BERT risiede nella sua capacità di elaborare il contesto bidirezionalmente, considerando sia le parole che precedono che quelle che seguono un determinato token durante l'addestramento.

\subsection{Fine-tuning e Trasferimento dell'Apprendimento}

Il fine-tuning consiste nell'adattare un modello pre-addestrato a un compito specifico, aggiornando i parametri del modello su un dataset dedicato. Questo processo permette di:

\begin{itemize}
    \item Specializzare il modello per domini o applicazioni specifiche
    \item Migliorare le performance su task particolari come classificazione, sentiment analysis, ecc.
    \item Adattare le rappresentazioni vettoriali agli schemi linguistici del dominio target
\end{itemize}

Nel nostro caso, il modello "Bert\_base\_fineTuned" è stato presumibilmente affinato per un compito specifico, modificando la struttura di base di BERT per ottimizzare le rappresentazioni vettoriali secondo le caratteristiche del dataset di addestramento.

\section{Metodologia di Estrazione degli Embedding}

\subsection{Caricamento del Modello e Tokenizer}

Il primo passo dell'analisi consiste nel caricamento del modello affinato da Hugging Face:

\begin{lstlisting}[language=Python]
model_name = "Teto03/Bert_base_fineTuned"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
\end{lstlisting}

Vengono importati sia il modello che il tokenizer associato. Il modello è di tipo \texttt{AutoModelForSequenceClassification}, indicando che è stato affinato specificamente per compiti di classificazione delle sequenze.

\subsection{Processo di Embedding delle Risposte}

L'estrazione degli embedding dalle risposte segue questi passaggi fondamentali:

\begin{enumerate}
    \item \textbf{Tokenizzazione}: Ogni risposta viene suddivisa in token secondo le regole del tokenizer BERT
    \item \textbf{Preparazione degli input}: Conversione dei token in tensori con l'aggiunta di token speciali [CLS] e [SEP]
    \item \textbf{Forward pass}: Elaborazione attraverso il modello con richiesta di output degli hidden states
    \item \textbf{Estrazione del vettore [CLS]}: Recupero del vettore corrispondente al token [CLS]
\end{enumerate}

Il codice specifico per l'estrazione è il seguente:

\begin{lstlisting}[language=Python]
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs, output_hidden_states=True)
embedding_vector = outputs.hidden_states[-1][:, 0, :].squeeze(0).detach().cpu().numpy()
\end{lstlisting}

La procedura evidenzia alcuni aspetti cruciali:

\begin{itemize}
    \item \textbf{Parameter \texttt{output\_hidden\_states=True}}: Richiede al modello di restituire tutti gli stati nascosti, non solo l'output finale
    \item \textbf{\texttt{hidden\_states[-1]}}: Seleziona l'ultimo layer nascosto, che contiene le rappresentazioni più raffinate
    \item \textbf{\texttt{[:, 0, :]}}: Estrae il vettore corrispondente al token [CLS] (posizione 0), che funge da rappresentazione dell'intera sequenza
    \item \textbf{\texttt{detach().cpu().numpy()}}: Converte il tensore PyTorch in un array NumPy per l'elaborazione successiva
\end{itemize}

\subsection{Caratteristiche del Vettore di Embedding}

Il vettore estratto dal token [CLS] ha dimensione \(d\), dove \(d\) è la dimensione dell'hidden state del modello BERT utilizzato. Nel caso di BERT base, \(d = 768\). Questo vettore funziona come una rappresentazione compatta dell'intera risposta, catturando le caratteristiche semantiche globali del testo analizzato.

La scelta di utilizzare il token [CLS] è particolarmente efficace poiché:

\begin{itemize}
    \item Durante il fine-tuning per la classificazione, il modello viene addestrato a condensare le informazioni rilevanti nel vettore [CLS]
    \item Rappresenta una sintesi dell'intero contesto, incorporando relazioni semantiche complesse
    \item Facilita l'elaborazione a valle come clustering o classificazione
\end{itemize}

\section{Analisi dei Risultati}

\subsection{Approccio di Clustering}

Per analizzare i pattern emergenti dagli embedding, è stato applicato un algoritmo di clustering K-Means con \(k=2\):

\begin{lstlisting}[language=Python]
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters = kmeans.fit_predict(X)
\end{lstlisting}

L'algoritmo K-Means identifica gruppi di risposte con caratteristiche semantiche simili, basandosi sulla prossimità dei loro vettori di embedding nello spazio a 768 dimensioni.

\subsection{Visualizzazione e Interpretazione dei Cluster}

Per visualizzare efficacemente i cluster in uno spazio bidimensionale, sono state utilizzate due tecniche di riduzione dimensionale:

\begin{enumerate}
    \item \textbf{Principal Component Analysis (PCA)}: Proiezione lineare che preserva la varianza globale
    \item \textbf{t-Distributed Stochastic Neighbor Embedding (t-SNE)}: Tecnica non lineare che preserva le relazioni di vicinanza locale
\end{enumerate}

\subsubsection{Analisi dei risultati PCA}

La visualizzazione PCA fornisce una prospettiva globale della distribuzione dei vettori di embedding:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{pca_visualization.png}
    \caption{Clustering dei vettori BERT visualizzati tramite PCA}
    \label{fig:pca}
\end{figure}

Dalla visualizzazione PCA emergono le seguenti osservazioni:

\begin{itemize}
    \item I due cluster mostrano una separazione apprezzabile, suggerendo che il modello BERT affinato riesce a distinguere efficacemente due categorie semantiche nelle risposte
    \item La disposizione dei punti lungo le componenti principali indica che le prime due componenti catturano una percentuale significativa della varianza totale
    \item La posizione dei centroidi (marcati con 'x') offre una rappresentazione sintetica del "contenuto semantico medio" di ciascun cluster
\end{itemize}

\subsubsection{Analisi dei risultati t-SNE}

La visualizzazione t-SNE fornisce una rappresentazione più dettagliata delle relazioni locali:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{tsne_visualization.png}
    \caption{Clustering dei vettori BERT visualizzati tramite t-SNE}
    \label{fig:tsne}
\end{figure}

L'analisi t-SNE rivela:

\begin{itemize}
    \item Una separazione più netta tra i cluster, con formazioni più compatte rispetto alla visualizzazione PCA
    \item Potenziali sottogruppi all'interno dei cluster principali, suggerendo sfumature semantiche ulteriori
    \item La presenza di outlier che potrebbero rappresentare risposte con caratteristiche linguistiche uniche
\end{itemize}

\subsection{Interpretazione dei Cluster Semantici}

I due cluster identificati rappresentano probabilmente gruppi di risposte con caratteristiche semantiche o stilistiche distintive. Senza analizzare il contenuto specifico delle risposte, possiamo ipotizzare che la separazione potrebbe essere basata su:

\begin{itemize}
    \item \textbf{Differenze tematiche}: Risposte che affrontano argomenti diversi
    \item \textbf{Variazioni stilistiche}: Differenze nel tono, formalità o struttura sintattica
    \item \textbf{Caratteristiche semantiche}: Polarità del sentimento, complessità concettuale, ecc.
\end{itemize}

\section{Considerazioni Tecniche}

\subsection{Vantaggi dell'Approccio Utilizzato}

L'utilizzo degli embedding del modello BERT affinato presenta numerosi vantaggi:

\begin{itemize}
    \item \textbf{Rappresentazioni contestuali}: Cattura relazioni semantiche complesse tra le parole
    \item \textbf{Transfer learning}: Sfrutta la conoscenza linguistica acquisita durante il pre-addestramento
    \item \textbf{Specializzazione del dominio}: Attraverso il fine-tuning, il modello è sensibile alle particolarità del dominio applicativo
    \item \textbf{Compattezza}: Il vettore [CLS] fornisce una rappresentazione efficiente dell'intera sequenza
\end{itemize}

\subsection{Limitazioni e Considerazioni}

È importante considerare alcune limitazioni dell'approccio:

\begin{itemize}
    \item \textbf{Perdita di informazione}: La riduzione a un singolo vettore [CLS] può perdere dettagli a livello di token
    \item \textbf{Determinismo del K-Means}: Il clustering K-Means è sensibile all'inizializzazione e presuppone cluster sferici
    \item \textbf{Interpretabilità}: I vettori di embedding sono intrinsecamente difficili da interpretare in termini linguistici diretti
    \item \textbf{Scelta arbitraria di k}: La selezione di 2 cluster potrebbe non riflettere la naturale segmentazione dei dati
\end{itemize}

\section{Proposte di Approfondimento}

Per estendere l'analisi, si propongono i seguenti approfondimenti:

\begin{enumerate}
    \item \textbf{Analisi semantica dei cluster}: Esaminare le risposte all'interno di ciascun cluster per identificare pattern tematici o stilistici
    \item \textbf{Ottimizzazione del numero di cluster}: Utilizzare metodi come l'elbow method o silhouette score per determinare il numero ottimale di cluster
    \item \textbf{Confronto tra layer}: Analizzare gli embedding estratti da layer diversi per comprendere l'evoluzione della rappresentazione semantica
    \item \textbf{Alternative al token [CLS]}: Valutare rappresentazioni alternative come la media degli embedding di tutti i token o tecniche di pooling più sofisticate
    \item \textbf{Confronto con altri modelli}: Comparare i risultati con quelli ottenuti da altri modelli linguistici come RoBERTa, DistilBERT o XLNet
\end{enumerate}

\section{Conclusioni}

L'analisi degli embedding generati dal modello BERT affinato ha evidenziato la capacità del modello di catturare relazioni semantiche complesse nelle risposte analizzate. Il processo di estrazione degli embedding, focalizzato sul token [CLS], fornisce una rappresentazione efficace dell'intera sequenza, facilitando l'identificazione di pattern semantici attraverso tecniche di clustering.

I risultati del clustering K-Means, visualizzati tramite PCA e t-SNE, hanno rivelato una chiara separazione tra due gruppi di risposte, suggerendo che il modello BERT affinato riesce a distinguere efficacemente categorie semantiche distinte. La differenza tra le visualizzazioni PCA e t-SNE evidenzia l'importanza di considerare sia relazioni globali che locali nell'interpretazione degli embedding.

Questo studio dimostra il potenziale dei modelli transformer affinati nell'analisi semantica dei testi e apre la strada a numerose applicazioni nell'ambito dell'elaborazione del linguaggio naturale, dalla classificazione automatica alla segmentazione tematica.

\begin{thebibliography}{9}

\bibitem{devlin2019bert}
Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4171-4186).

\bibitem{wolf2020transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... \& Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38-45).

\bibitem{reimers2019sentencebert}
Reimers, N., \& Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (pp. 3982-3992).

\bibitem{maaten2008visualizing}
Van der Maaten, L., \& Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605.

\end{thebibliography}

\end{document}